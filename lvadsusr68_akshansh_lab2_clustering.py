# -*- coding: utf-8 -*-
"""LVADSUSR68_Akshansh_LAB2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/143QooWm-JrEP24r2PnB8ztCgyhwBIC2b
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Load the dataset
data = pd.read_csv('Mall_Customers.csv')

# Display the first few rows of the dataset
print("Original dataset:")
print(data.head())

# Check for missing values
print("\nMissing values before imputation:")
print(data.isnull().sum())

# Separate numerical and categorical columns
numerical_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
categorical_cols = ['Gender']

# Preprocessing pipeline
numerical_transformer = SimpleImputer(strategy='mean')
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Apply preprocessing
data_preprocessed = pd.DataFrame(preprocessor.fit_transform(data))

# Replace missing values with mean (only for numerical columns)
data_preprocessed.fillna(data_preprocessed.mean(), inplace=True)

# Data normalization
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_preprocessed)

# EDA statistics
print("\nEDA statistics:")
print(data_preprocessed.describe())

# EDA visualization
# Pairplot of selected features
sns.pairplot(data_preprocessed)
plt.show()

# Determine the optimal number of clusters using the elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(data_scaled)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Determine the optimal number of clusters using silhouette score
silhouette_scores = []
for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(data_scaled)
    silhouette_avg = silhouette_score(data_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)
optimal_n_clusters = silhouette_scores.index(max(silhouette_scores)) + 2
print("Optimal number of clusters (Silhouette score):", optimal_n_clusters)


# Apply k-means clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_n_clusters, init='k-means++', random_state=42)
cluster_labels = kmeans.fit_predict(data_scaled)
data['Cluster'] = cluster_labels

# Cluster profiling
cluster_profile = data.groupby('Cluster').mean()
print("\nCluster profile:")
print(cluster_profile)

# Feature engineering: Add new feature - Ratio of Spending to Income
data_preprocessed['Spending_Income_Ratio'] = data_preprocessed[2] / data_preprocessed[1]

# Show clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data_preprocessed, x=data_preprocessed[2], y=data_preprocessed[1], hue=data['Cluster'], palette='viridis')
plt.xlabel('Spending Score')
plt.ylabel('Annual Income (k$)')
plt.title('Cluster Visualization')
plt.legend(title='Cluster')
plt.show()